{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics\n",
    "We can create a basic dataset using from array like objsects (tf tensors, numpy arrays, lists, tuples, even pandas dataframes). The first dimension of the input will be removed and used as the dataset dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (2,), types: tf.int64>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(np.arange(6).reshape((3,2)))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dataset` is an iterable so we can loop through its elements by creating an *iterator* and calling the *next* element. This can be easily (and transparently) done using a `for` loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1], shape=(2,), dtype=int64)\n",
      "tf.Tensor([2 3], shape=(2,), dtype=int64)\n",
      "tf.Tensor([4 5], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: `__iter__` works with python directives, thus only in Eager mode.\n",
    "\n",
    "The element are tensors, if we are interested in their value only we can use the `as_numpy_iterator()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10) # equivalent to tf.data.Dataset.from_tensor_slices(np.arange(10))\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform some actions on a Dataset, for example using the `shuffle()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 0, 2, 5, 6, 8, 4, 9, 7]\n",
      "[1, 3, 2, 0, 5, 6, 8, 7, 4, 9]\n"
     ]
    }
   ],
   "source": [
    "shuffled_dt = dataset.shuffle(3)\n",
    "print(list(shuffled_dt.as_numpy_iterator()))\n",
    "print(list(shuffled_dt.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every time a new iterator is created, the data are shuffled.\n",
    "\n",
    "*Note*: The transformation does not happen in place but a new `Dataset` is returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also **batch** the elements of a dataset together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 1, 3]), array([4, 5, 2]), array([8, 9, 6]), array([7])]\n",
      "[array([0, 2, 3]), array([5, 1, 4]), array([7, 8, 6]), array([9])]\n"
     ]
    }
   ],
   "source": [
    "batched_dt = shuffled_dt.batch(3)\n",
    "print(list(batched_dt.as_numpy_iterator()))\n",
    "print(list(batched_dt.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply a function element-wise with the `map()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 13, 20, 9]\n"
     ]
    }
   ],
   "source": [
    "def fn(x): return tf.reduce_sum(x)\n",
    "\n",
    "transformed_dt = batched_dt.map(fn)\n",
    "\n",
    "print(list(transformed_dt.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `cache()` method we cache the data in memory. This is done after the first (complete) iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 0]\n",
      "[2, 3, 1, 0]\n",
      "[0, 1, 4, 5, 6, 7, 3, 9, 8, 2]\n",
      "[0, 1, 4, 5, 6, 7, 3, 9, 8, 2]\n"
     ]
    }
   ],
   "source": [
    "cached_dataset = shuffled_dt.cache()\n",
    "print(list(cached_dataset.take(4).as_numpy_iterator())) # Not cached yet\n",
    "print(list(cached_dataset.take(4).as_numpy_iterator())) # Not cached yet\n",
    "print(list(cached_dataset.as_numpy_iterator())) # Cached\n",
    "print(list(cached_dataset.as_numpy_iterator())) # The same!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So... Shuffle after caching!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = 'data/dataset/'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "for i in range(6):\n",
    "    N_el = 3\n",
    "    x1 = np.ones(shape=(N_el,1))*i\n",
    "    x2 = np.arange(N_el).reshape((N_el,1))\n",
    "    y  = np.random.rand(N_el,1)\n",
    "    table = np.hstack([x1,x2,y])\n",
    "    filename = os.path.join(data_dir,\"data_%d.csv\"%i)\n",
    "    np.savetxt(filename, table, delimiter=\",\", header=\"x1,x2,y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_0.csv data_1.csv data_2.csv data_3.csv data_4.csv data_5.csv\r\n"
     ]
    }
   ],
   "source": [
    "! ls {data_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# x1,x2,y\r\n",
      "5.000000000000000000e+00,0.000000000000000000e+00,1.510211592071286635e-02\r\n",
      "5.000000000000000000e+00,1.000000000000000000e+00,6.108007938288444461e-01\r\n",
      "5.000000000000000000e+00,2.000000000000000000e+00,3.874759526742046489e-01\r\n"
     ]
    }
   ],
   "source": [
    "! cat {filename}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset of file paths\n",
    "We can create a dataset iterating through the filenames.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'data/dataset/data_0.csv',\n",
       " b'data/dataset/data_4.csv',\n",
       " b'data/dataset/data_2.csv',\n",
       " b'data/dataset/data_1.csv',\n",
       " b'data/dataset/data_3.csv',\n",
       " b'data/dataset/data_5.csv']"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_dt = tf.data.Dataset.list_files(data_dir+\"*\")\n",
    "list(filename_dt.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset comprising lines from files\n",
    "More importantly we can create a dataset that iterates through the lines of a **single file**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: data/dataset/data_5.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[b'# x1,x2,y',\n",
       " b'5.000000000000000000e+00,0.000000000000000000e+00,1.510211592071286635e-02',\n",
       " b'5.000000000000000000e+00,1.000000000000000000e+00,6.108007938288444461e-01',\n",
       " b'5.000000000000000000e+00,2.000000000000000000e+00,3.874759526742046489e-01']"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Filename:\", filename)\n",
    "single_file_dt = tf.data.TextLineDataset(filename)\n",
    "list(single_file_dt.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yuck!** There is the header! As it is the first line, we can skip it easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'5.000000000000000000e+00,0.000000000000000000e+00,1.510211592071286635e-02',\n",
       " b'5.000000000000000000e+00,1.000000000000000000e+00,6.108007938288444461e-01',\n",
       " b'5.000000000000000000e+00,2.000000000000000000e+00,3.874759526742046489e-01']"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_header_dt = single_file_dt.skip(1)\n",
    "list(no_header_dt.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same API we can create a dataset iterating through lines from multiple files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'# x1,x2,y',\n",
       " b'1.000000000000000000e+00,0.000000000000000000e+00,1.761952109868600846e-01',\n",
       " b'1.000000000000000000e+00,1.000000000000000000e+00,2.937346730679679663e-02',\n",
       " b'1.000000000000000000e+00,2.000000000000000000e+00,3.545499146055860473e-01',\n",
       " b'# x1,x2,y',\n",
       " b'4.000000000000000000e+00,0.000000000000000000e+00,4.467986475199151597e-01']"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_file_dt = tf.data.TextLineDataset(filename_dt)\n",
    "list(multi_file_dt.as_numpy_iterator())[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the header is repeated several time across the dataset. To remove it we can filter out all lines starting with \"#\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'1.000000000000000000e+00,0.000000000000000000e+00,1.761952109868600846e-01',\n",
       " b'1.000000000000000000e+00,1.000000000000000000e+00,2.937346730679679663e-02',\n",
       " b'1.000000000000000000e+00,2.000000000000000000e+00,3.545499146055860473e-01',\n",
       " b'2.000000000000000000e+00,0.000000000000000000e+00,1.222223120351877412e-01',\n",
       " b'2.000000000000000000e+00,1.000000000000000000e+00,8.895071154196134700e-01',\n",
       " b'2.000000000000000000e+00,2.000000000000000000e+00,6.112288628589750417e-01',\n",
       " b'3.000000000000000000e+00,0.000000000000000000e+00,6.146688086690119679e-01']"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_header_dt = multi_file_dt.filter(lambda line: tf.not_equal(tf.strings.substr(line, 0, 1), \"#\"))\n",
    "list(no_header_dt.as_numpy_iterator())[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interleave\n",
    "\n",
    "In the previous dataset the lines were iterated sequentially for each file. If were dealing with huge files this behaviour could affect our ability to randomise the content of the dataset. Using `interleave()` instead we can iterate through groups of files, take a block of lines from each file in the group, and stack them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'3.000000000000000000e+00,0.000000000000000000e+00,6.146688086690119679e-01',\n",
       " b'5.000000000000000000e+00,1.000000000000000000e+00,6.108007938288444461e-01',\n",
       " b'5.000000000000000000e+00,2.000000000000000000e+00,3.874759526742046489e-01',\n",
       " b'2.000000000000000000e+00,1.000000000000000000e+00,8.895071154196134700e-01',\n",
       " b'3.000000000000000000e+00,2.000000000000000000e+00,9.307465018518346067e-01',\n",
       " b'5.000000000000000000e+00,0.000000000000000000e+00,1.510211592071286635e-02',\n",
       " b'2.000000000000000000e+00,2.000000000000000000e+00,6.112288628589750417e-01',\n",
       " b'2.000000000000000000e+00,0.000000000000000000e+00,1.222223120351877412e-01',\n",
       " b'4.000000000000000000e+00,0.000000000000000000e+00,4.467986475199151597e-01',\n",
       " b'0.000000000000000000e+00,0.000000000000000000e+00,1.400779891641089625e-01',\n",
       " b'1.000000000000000000e+00,1.000000000000000000e+00,2.937346730679679663e-02',\n",
       " b'1.000000000000000000e+00,0.000000000000000000e+00,1.761952109868600846e-01',\n",
       " b'0.000000000000000000e+00,1.000000000000000000e+00,1.599590417554552779e-01',\n",
       " b'4.000000000000000000e+00,2.000000000000000000e+00,2.205462898738513866e-01',\n",
       " b'0.000000000000000000e+00,2.000000000000000000e+00,3.026207561417123548e-01',\n",
       " b'3.000000000000000000e+00,1.000000000000000000e+00,8.132554020460818212e-01',\n",
       " b'1.000000000000000000e+00,2.000000000000000000e+00,3.545499146055860473e-01',\n",
       " b'4.000000000000000000e+00,1.000000000000000000e+00,8.841140314413962198e-01']"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interleave_dt = filename_dt.interleave(lambda x: tf.data.TextLineDataset(x).skip(1),\n",
    "                                       cycle_length=3,\n",
    "                                       block_length=2).shuffle(5)\n",
    "list(interleave_dt.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding bytes\n",
    "To read the above lines and decode the numeric data we should map the `tf.io.decode_csv()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([2., 1.], dtype=float32), 0.8895071),\n",
       " (array([4., 1.], dtype=float32), 0.884114),\n",
       " (array([1., 0.], dtype=float32), 0.1761952),\n",
       " (array([2., 0.], dtype=float32), 0.12222231),\n",
       " (array([4., 2.], dtype=float32), 0.22054629),\n",
       " (array([4., 0.], dtype=float32), 0.44679865),\n",
       " (array([1., 2.], dtype=float32), 0.3545499),\n",
       " (array([0., 0.], dtype=float32), 0.140078),\n",
       " (array([2., 2.], dtype=float32), 0.6112289),\n",
       " (array([1., 1.], dtype=float32), 0.029373467),\n",
       " (array([5., 0.], dtype=float32), 0.015102115),\n",
       " (array([0., 2.], dtype=float32), 0.30262077),\n",
       " (array([5., 2.], dtype=float32), 0.38747597),\n",
       " (array([3., 2.], dtype=float32), 0.9307465),\n",
       " (array([3., 1.], dtype=float32), 0.8132554),\n",
       " (array([0., 1.], dtype=float32), 0.15995905),\n",
       " (array([5., 1.], dtype=float32), 0.6108008),\n",
       " (array([3., 0.], dtype=float32), 0.6146688)]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode_record(line):\n",
    "    record = tf.io.decode_csv(line, record_defaults=[0.,0.,0.])\n",
    "    x = tf.stack(record[:-1])\n",
    "    y = tf.stack(record[-1])\n",
    "    return x, y\n",
    "\n",
    "decoded_dt = interleave_dt.map(decode_record)\n",
    "list(decoded_dt.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
