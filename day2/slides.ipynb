{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2,3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **Day 2: Improve performances**\n",
    "    - TF execution modes\n",
    "    - GPUs support\n",
    "    - Improve performances with input data pipelines\n",
    "    - Parallel execution across multiple devices\n",
    "    - Distribution strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Eager Execution\n",
    "Tensorflow 2 by default executes operations in eager mode, an imperative programming environment that evaluates immediately, without building graphs.\n",
    "\n",
    "There are several advantages about the eager execution:\n",
    "* It is easy, it looks like Numpy (and the two are nicely integrated)\n",
    "* You can always check what's going on printing stuff\n",
    "* You can operate on tensor using python control flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager execution: True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Eager execution:\", tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_tf: tf.Tensor(\n",
      "[[0. 1. 2.]\n",
      " [3. 4. 5.]], shape=(2, 3), dtype=float32)\n",
      "y_tf: tf.Tensor(\n",
      "[[  1.          2.7182817   7.389056 ]\n",
      " [ 20.085537   54.59815   148.41316  ]], shape=(2, 3), dtype=float32)\n",
      "z_tf == y_tf ? True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_np = np.arange(6, dtype=np.float32).reshape((2,3))\n",
    "x_tf = tf.constant(x_np)\n",
    "print(\"x_tf:\",x_tf)\n",
    "\n",
    "y_tf = tf.math.exp(x_tf)\n",
    "print(\"y_tf:\",y_tf)\n",
    "\n",
    "z_tf = tf.math.exp(x_np)\n",
    "print(\"z_tf == y_tf ?\", np.all(z_tf == y_tf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can use python control flow even while tracing operations with `tf.GradientTape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\t tf.Tensor([[0.3 0.5]], shape=(1, 2), dtype=float32)\n",
      "y:\t tf.Tensor([0.4], shape=(1,), dtype=float32)\n",
      "dy/dx:\t tf.Tensor([[0.5 0.5]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def cond_op(x):\n",
    "    if tf.reduce_any(x > 1): # Conditional\n",
    "        return tf.reduce_max(x, axis=0) # First axis reduced\n",
    "    else:\n",
    "        return tf.reduce_mean(x, axis=-1) # Last axis reduced\n",
    "\n",
    "x = tf.constant([[.3, .5]])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = cond_op(x)\n",
    "\n",
    "print('x:\\t', x)\n",
    "print('y:\\t', y)\n",
    "print('dy/dx:\\t', tape.gradient(y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Eager execution makes development and debugging more interactive but this can come at the expense of performance and deployability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Graph Mode\n",
    "\n",
    "<img style=\"float: left;\" src=\"https://miro.medium.com/max/504/1*SmfhKWHXHVEMg8KqNaj-uw.gif\">\n",
    "Opposed to the imperative environment offered by the eager execution there is the graph mode. In this mode symbolic graphs are used to represent the computations where each node corresponds to an operation and data flow following the arrows.\n",
    "\n",
    "\n",
    "The benefits of graph mode are numerous, and can be summarized in:\n",
    "- **Performance**: Computations and memory usage can be optimized.\n",
    "- **Portability**: The dataflow graph is a language-independent representation of the code in your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### tf.function\n",
    "TensorFlow 2 makes it easy to convert a Python program to a symbolic graph. With the `tf.function` API a function can be compiled into a callable TensorFlow graph (usually called *TensoFlow Function*).\n",
    "\n",
    "**Note:** Keras APIs are already integrated with `tf.function`. When you call the `fit()` method on a model the execution is run by default in graph mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.simple_fn(x)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simple_fn(x):\n",
    "    y = x * 2\n",
    "    return y\n",
    "\n",
    "simple_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.def_function.Function at 0x7f48098007f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_simple_fn = tf.function(simple_fn)\n",
    "tf_simple_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Alternatively you can decorate the function definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.def_function.Function at 0x7f48098006a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.function\n",
    "def tf_other_fn(x):\n",
    "    return x**2 + 1\n",
    "\n",
    "tf_other_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "TF functions can be used exactly as Python functions: you can execute it eagerly, you can compute gradients, you can even call them with arguments of different types and shapes.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor([ 0.6 -2.4  2. ], shape=(3,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "x = 4\n",
    "print(tf_simple_fn(x))\n",
    "\n",
    "x = tf.constant(5)\n",
    "print(tf_simple_fn(x))\n",
    "\n",
    "x = tf.constant([.3, -1.2, 1], dtype=tf.float64)\n",
    "print(tf_simple_fn(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "polymorphism.. WAT?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "TensorFlow graphs are statically typed. Thus, every time a TF function is called with a new *input signature* (combination of arguments shapes and types) a new *concrete function* (a graph with a single signature) is created. Given an input we can get the concrete function corresponding to the input signature with the `get_concrete_function()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.eager.function.ConcreteFunction object at 0x7f480983ca00>\n",
      "<tensorflow.python.eager.function.ConcreteFunction object at 0x7f4809896d00>\n",
      "<tensorflow.python.eager.function.ConcreteFunction object at 0x7f48b7f3a250>\n"
     ]
    }
   ],
   "source": [
    "x = 4\n",
    "print(tf_simple_fn.get_concrete_function(x))\n",
    "\n",
    "x = 3\n",
    "print(tf_simple_fn.get_concrete_function(x))\n",
    "\n",
    "x = tf.constant(5)\n",
    "print(tf_simple_fn.get_concrete_function(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This holds for Tensor arguments only. For example, every instance of a Python numeric type will generate a new graph! To avoid this, pass numeric arguments as Tensors whenever possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The underlying graph of a concrete function can be accessed via the `graph` attribute. Each graph contains a set of `tf.Operation` objects, which represent units of computation (the nodes in the graph). The `get_operations()` returns the list of operations constituting a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'x' type=Placeholder>,\n",
       " <tf.Operation 'mul/y' type=Const>,\n",
       " <tf.Operation 'mul' type=Mul>,\n",
       " <tf.Operation 'Identity' type=Identity>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([3.])\n",
    "g = tf_simple_fn.get_concrete_function(x).graph\n",
    "g.get_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"mul/y\"\n",
      "op: \"Const\"\n",
      "attr {\n",
      "  key: \"dtype\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"value\"\n",
      "  value {\n",
      "    tensor {\n",
      "      dtype: DT_FLOAT\n",
      "      tensor_shape {\n",
      "      }\n",
      "      float_val: 2.0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(g.get_operations()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'Const' type=Const>, <tf.Operation 'Identity' type=Identity>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = tf_simple_fn.get_concrete_function(5).graph\n",
    "g.get_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Tracing\n",
    "\n",
    "When the argument has a new signature, a *symbolic tensor* (a tensor with name, shape and type but without values) is passed to the function and every `tf` operation encountered adds a `tf.Operation` node to a new graph.\n",
    "\n",
    " In particular, this means that every call to an external library (even Numpy or the standard library) will be executed **only during tracing** and will not be added in the graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing! Argument: Tensor(\"x:0\", shape=(1,), dtype=float32)\n",
      "Executing! Argument: [3]\n",
      "Executing! Argument: [1.5]\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def print_fn(x):\n",
    "    print(\"Tracing! Argument:\", x)\n",
    "    tf.print(\"Executing! Argument:\", x)\n",
    "\n",
    "print_fn(tf.constant([3], dtype=tf.float32))\n",
    "print_fn(tf.constant([1.5], dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing! Argument: 1\n",
      "Executing! Argument: 1\n",
      "Tracing! Argument: 2\n",
      "Executing! Argument: 2\n",
      "Tracing! Argument: Tensor(\"x:0\", shape=(1,), dtype=int32)\n",
      "Executing! Argument: [3]\n",
      "Executing! Argument: [-2]\n"
     ]
    }
   ],
   "source": [
    "print_fn(1)\n",
    "print_fn(2)\n",
    "\n",
    "print_fn(tf.constant([3]))\n",
    "print_fn(tf.constant([-2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The different behaviour between `print` and `tf.print` turns out to be very helpful when tracking down issues that only appear within `tf.function`. But remember to be very carefull when calling an external library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing - Sampled: 666\n",
      "Executing - Sampled: 666\n",
      "Executing - Sampled: 666\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def my_rand():\n",
    "    x = np.random.randint(1000)\n",
    "    print(\"Tracing - Sampled:\", x)\n",
    "    tf.print(\"Executing - Sampled:\", x)\n",
    "    \n",
    "my_rand()\n",
    "my_rand()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### AutoGraph\n",
    "AutoGraph is another component that comes in action with `tf.function` and performs a first conversion of plain Python into TensorFlow code:\n",
    "\n",
    "- `for`/`while` -> `tf.while_loop` (break and continue are supported)\n",
    "- `if` -> `tf.cond`\n",
    "\n",
    "This allows to trace data dependent control flows in the code and make them dynamically operate at execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing - True - Argument: Tensor(\"x:0\", shape=(2, 1), dtype=float32)\n",
      "Tracing - False  - Argument: Tensor(\"x:0\", shape=(2, 1), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.5], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.function(autograph=True) # default: autograph=True\n",
    "def tf_cond_fn(x):\n",
    "    if tf.shape(x)[0] > 1: # Conditional\n",
    "        print(\"Tracing - True - Argument:\", x)\n",
    "        return tf.reduce_max(x, axis=0)\n",
    "    else:\n",
    "        print(\"Tracing - False  - Argument:\", x)\n",
    "        return tf.reduce_mean(x, axis=-1)\n",
    "\n",
    "tf_cond_fn(tf.constant([[0.3], [0.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Variables\n",
    "Creating a new `Variable` in a function works in eager mode but not in graph mode!\n",
    ">Due to tracing semantics, `tf.function` will reuse the same variable each call, but eager mode will create a new variable with each call. To guard against this mistake, tf.function will raise an error if it detects dangerous variable creation behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def tf_new_var(x):\n",
    "    v = tf.Variable(0.)\n",
    "    v.assign_add(x)\n",
    "    return v\n",
    "\n",
    "# tf_new_var(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Summary\n",
    "- Debug in Eager mode, it is easier.\n",
    "- Then decorate with `@tf.function` to get performant and portable models.\n",
    "\n",
    "To convert python function to TF function remeber:\n",
    "1. Be aware of using external libraries. Try to use `tf` methods evrywhere!\n",
    "2. Don't rely on Python side effects like object mutation or list appends.\n",
    "3. Variables creation in a TF function is allowed only on the first call. Avoid it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# GPU support\n",
    "\n",
    "Since version 2.1 TensorFlow have a unique installation procedure for both CPU-only and GPU machines. Nevertheless, the GPU support has both hardware (NVIDIA GPU only) and software requirements (NVIDIA drivires, CUDA 10.1, cuDNN 7.6).\n",
    "\n",
    "If the above requirements are satisfied, TensorFlow code, and tf.keras models will transparently run on a single GPU with **no code changes**.\n",
    "\n",
    "You can check the availabel devices via `tf.config.experimental.get_visible_devices()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.get_visible_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By default, every TensorFlow operation and every tensor that have a GPU implementation (usually referred to as GPU kernels) will be placed on the first GPU (\"GPU:0\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/job:localhost/replica:0/task:0/device:GPU:0'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable([3., 2.], [1., 7.])\n",
    "x.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If there are no GPU kernels for the variable/operation it will placed on the CPU. This is the case, for example, for integer tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/job:localhost/replica:0/task:0/device:CPU:0'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable([3, 2], [1, 7])\n",
    "x.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out which devices your operations and tensors are assigned to, you can use `tf.debugging.set_log_device_placement(True)` as the first statement of your program. It will print placement logging for any Tensor allocations or operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, when possible, the default behaviour can be overridden by explicitly placing tensors and operations on specific devices using the tf.device context manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/job:localhost/replica:0/task:0/device:CPU:0'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    x = tf.Variable([3., 2.], [1., 7.])\n",
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/job:localhost/replica:0/task:0/device:CPU:0'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    x = tf.Variable([3, 2], [1, 7])\n",
    "x.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "As you can see, in the last case our demand has not been satisfied. You may wish to get an error. If this is the case set `tf.config.set_soft_device_placement(False)` at the very begging of your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By default, at the first computation, TensorFlow grabs nearly all of the memory of all GPUs visible to the process. This means that if you try to run a second TensorFlow program on the same machine you wil most probably get an error. There are a couple of things you can do to avoid this problem:\n",
    "\n",
    "- If you have multiple GPUs available, you can limit TensorFlow to a specific set of GPUs by:\n",
    "    1. setting the `CUDA_VISIBLE_DEVICES` environment variable (before any computation).\n",
    "    2. using the `tf.config.experimental.set_visible_devices()` method.\n",
    "    \n",
    "- To controll the memory allocated by a TensorFlow program:\n",
    "    1. with `tf.config.experimental.set_memory_growth` the process attempts to allocate only as much GPU memory as needed.\n",
    "    2. You can create a virtual device with `tf.config.set_logical_device_configuration` and set its memory limit.\n",
    "    \n",
    "You can even simulate multiple GPUs by creating multiple logical devices from a single phisical GPU. Check the [docs](https://www.tensorflow.org/guide/gpu)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# tf.data\n",
    "When training models on large datasets, as often is the case in Deep Learning, the bottleneck to reduce training time moves from the computations required by the model to deliver the data to the computing devices. Especially when working with accellerators it is crucial to provide them the next batch before the current train step has finished to avoid idle time.\n",
    "\n",
    "For this purpose the TensorFlow module `tf.data` provides a set of APIs that enables you to build complex input pipelines from simple, reusable pieces. In particular, it makes it possible to handle large amounts of data, read from different data formats, and perform complex transformations in a flexible and efficient way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dataset\n",
    "The main abstraction in `tf.data` is `Dataset`. This API is used to represents a potentially large sequence of elements, in which each element consists of one or more components. Nevertheless, all elements must have the same (nested) structure.\n",
    "\n",
    "`Dataset` usage follows a common pattern:\n",
    "1. Create a source dataset from your input data.\n",
    "2. Apply dataset transformations to preprocess the data.\n",
    "3. Iterate over the dataset and process the elements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are different ways to create a dataset from source:\n",
    "- If the input data fits in memory, `Dataset.from_tensor_slices` creates datasets from array-like objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (2,), types: tf.int64>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(np.arange(20).reshape(-1,2))\n",
    "dataset # tensors are sliced along their first dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `Dataset.list_files` creates a dataset of all files matching a pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ShuffleDataset shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.list_files('./*')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To process lines from files, use `TextLineDataset`.\n",
    "- If the files are written in `TFRecord` format, use `TFRecordDataset`.\n",
    "- As always.. Much more in the [docs](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Iterable object\n",
    "In particular, `Dataset` objects are Python iterables. This makes it possible to consume its elements using a for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'./slides.ipynb', shape=(), dtype=string)\n",
      "tf.Tensor(b'./3.Dataset_basics.ipynb', shape=(), dtype=string)\n",
      "tf.Tensor(b'./README.md', shape=(), dtype=string)\n",
      "tf.Tensor(b'./4.Dataset_performance.ipynb', shape=(), dtype=string)\n",
      "tf.Tensor(b'./5.Dataset_images.ipynb', shape=(), dtype=string)\n",
      "tf.Tensor(b'./__pycache__', shape=(), dtype=string)\n",
      "tf.Tensor(b'./Untitled.ipynb', shape=(), dtype=string)\n",
      "tf.Tensor(b'./utils.py', shape=(), dtype=string)\n",
      "tf.Tensor(b'./.ipynb_checkpoints', shape=(), dtype=string)\n",
      "tf.Tensor(b'./6.ResNet_distributed.ipynb', shape=(), dtype=string)\n",
      "tf.Tensor(b'./model_toy.py', shape=(), dtype=string)\n",
      "tf.Tensor(b'./1.Eager_Graph_benchmarks.ipynb', shape=(), dtype=string)\n",
      "tf.Tensor(b'./0.ResNet_custom_training.ipynb', shape=(), dtype=string)\n",
      "tf.Tensor(b'./2.GPU_support.ipynb', shape=(), dtype=string)\n",
      "tf.Tensor(b'./model_ResNetv2.py', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.list_files('./*')\n",
    "for tensor in dataset:\n",
    "    print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Equivalently, we can create an iterator via the Python built-in `iter` and get the next element by calling `next`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'./model_toy.py'>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_iterator = iter(dataset)\n",
    "next(ds_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "An alternative way to inspect the content of the dataset is the `as_numpy_iterator()` method. It returns an iterator which converts all elements of the dataset to numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'./2.GPU_support.ipynb',\n",
       " b'./.ipynb_checkpoints',\n",
       " b'./model_toy.py',\n",
       " b'./4.Dataset_performance.ipynb',\n",
       " b'./Untitled.ipynb',\n",
       " b'./model_ResNetv2.py',\n",
       " b'./slides.ipynb',\n",
       " b'./3.Dataset_basics.ipynb',\n",
       " b'./README.md',\n",
       " b'./0.ResNet_custom_training.ipynb',\n",
       " b'./6.ResNet_distributed.ipynb',\n",
       " b'./__pycache__',\n",
       " b'./utils.py',\n",
       " b'./1.Eager_Graph_benchmarks.ipynb',\n",
       " b'./5.Dataset_images.ipynb']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transformations\n",
    "Another way to construct a datasets from one or more `Dataset` objects is via data transformations. There are many possible transformations, the most common are:\n",
    "\n",
    "- `map()` applies a function to each element in the dataset.\n",
    "- `filter()` filters a dataset according to a predicate.\n",
    "- `batch()` combines consecutive elements of this dataset into batches.\n",
    "- `repeat()` repeats a dataset, possibly infinite times.\n",
    "- `shuffle()` randomly shuffles the elements.\n",
    "\n",
    "We will see some of those in action in the notebook [2.Dataset_basics](./2.Dataset_basics.ipynb) while the notebook [3.Dataset_performance.ipynb](3.Dataset_performance.ipynb) discusses performance improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multiple devices\n",
    "\n",
    "There are several reasons to use multiple devices in Deep Learning:\n",
    "\n",
    "- To run the data preprocessing on the CPU and the compute intense part on a GPU.\n",
    "- To train the same model simultaneuosly on different devices to tune its hyper-parameters.\n",
    "- To train different models on differnt devices and then collect the results in an ensemble model.\n",
    "- To speed-up the training of a model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training a model on multiple devices\n",
    "\n",
    "There are two main approaches to distributed training:\n",
    "\n",
    "- **Model parallelism**: The model is split in chunks and each chunk is assigned to a different device. \n",
    "- **Data parallelism**: The model is replicated across teh devices and each device process a chunk of data.\n",
    "![decomposition_dimensions](https://cdn.filestackcontent.com/a5diV7cSQ9iM1m8OvMRu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model parallelism\n",
    "The efficiency of model parallelism depends **strongly** on the network architecture! It can be performed on two dimensions:\n",
    "- **Layer parallelism**: Each device has a consecutive block of layers. In the picture each colour represent a different GPU.\n",
    "![layer_parallelism](https://secureservercdn.net/198.12.145.239/a7b.fcb.myftpupload.com/wp-content/uploads/2020/03/Picture1s-9_1.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Vertical Split**: Layers are split on multiple devices.\n",
    "![vertical_parallelism](https://docs.chainer.org/en/stable/_images/parallelism.png)\n",
    "- **Hybrid methods**:\n",
    "![](https://fananymi.files.wordpress.com/2015/03/dlbig.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data parallelism\n",
    "\n",
    "Model parallelism can be unavoidable when the model is really big but when it fit in each device memory there is no much sense in splitting it. A different approach is to split the data: each device has its own copy of the model (called **replica**), it receives a minibatch and via backpropagation computes the weights updates. The update can be performed in two ways:\n",
    "\n",
    "- **Collective AllReduce** - Each device receives the average update and perform a **synchronous update** . \n",
    "\n",
    "- Comunicating to a **paramenter server** - A special worker is designed to receive the updates and send back to the devices the new weights. \n",
    "\n",
    "With a parameter server there is no need to wait for all devices and teh weights updates can be performed **asynchronously**. In particular this means that the replicas will have different weights!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Distribution strategies\n",
    "The `tf.distribute.Strategy` is an API designed to distribute training accross multiple devices. Althought it doesn't cover all the use cases, it has the benefit to be really easy to use and to switch between strategies.\n",
    "\n",
    "It can be used to distribute the execution of Keras APIs and custom training loops. Moreovoer, it works in both eager and graph mode (although it works best when used with `tf.function`), and it does not require massive code changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Epoch 1/2\n",
      "INFO:tensorflow:batch_all_reduce: 27 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 27 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 2.2346\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 2.2008\n"
     ]
    }
   ],
   "source": [
    "from model_toy import get_toy_ResNet\n",
    "\n",
    "N_train = 60000\n",
    "x_train = tf.random.uniform([N_train,32,32,3])\n",
    "y_train = tf.random.uniform([N_train,1], minval=0, maxval=9, dtype=tf.int32)\n",
    "\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with mirrored_strategy.scope():\n",
    "    dist_model = get_toy_ResNet()\n",
    "    dist_model.compile(loss='sparse_categorical_crossentropy', optimizer=\"RMSProp\")\n",
    "    \n",
    "history = dist_model.fit(x=x_train, y=y_train, batch_size=32, epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:ParameterServerStrategy (CentralStorageStrategy if you are using a single machine) with compute_devices = ['/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1'], variable_device = '/device:CPU:0'\n",
      "Epoch 1/2\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 2.2351\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 2.2007\n"
     ]
    }
   ],
   "source": [
    "central_storage_strategy = tf.distribute.experimental.CentralStorageStrategy()\n",
    "\n",
    "with central_storage_strategy.scope():\n",
    "    dist_model = get_toy_ResNet()\n",
    "    dist_model.compile(loss='sparse_categorical_crossentropy', optimizer=\"RMSProp\")\n",
    "    \n",
    "history = dist_model.fit(x=x_train, y=y_train, batch_size=32, epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise\n",
    "\n",
    "In the file [model_ResNetv2.py](model_ResNetv2.py) you can find an accurate implementation of the ResNet v2 (from [here](https://keras.io/examples/cifar10_resnet/)). Create a `tf.Dataset` containing CIFAR10, apply to it some basic image preprocessing (avoid rotation!) and train a ResNet56_v2 model on it for at least 30 epochs. Distribute the training over multiple GPUs (try at least with 2) and test different batch sizes. How does the time per epoch change? What about the validation accuracy?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
